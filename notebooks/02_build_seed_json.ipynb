{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üå± GifLab Seed Builder - Your Optimization Assistant\n",
        "\n",
        "**A step-by-step guide to creating smart data files that make your GIF processing faster and more efficient**\n",
        "\n",
        "## üëã Welcome to the Seed Builder!\n",
        "\n",
        "### üéØ What This Notebook Does:\n",
        "This notebook creates special \"seed files\" that act like a smart assistant for your GIF processing. Think of it as creating a detailed catalog and instruction manual for your GIF collection.\n",
        "\n",
        "### ü§î Why Do I Need This?\n",
        "- **Makes processing much faster** - Skip unnecessary work\n",
        "- **Enables smart resume** - Pick up exactly where you left off if interrupted\n",
        "- **Optimizes resource usage** - Use your computer's power more efficiently\n",
        "- **Provides intelligent recommendations** - Get the best settings for each GIF\n",
        "\n",
        "### üìÅ What Files Will Be Created:\n",
        "1. **`lookup_seed_metadata.json`** - A detailed catalog of all your GIFs\n",
        "2. **`lookup_seed_processing.json`** - Smart recommendations for compression settings\n",
        "3. **`lookup_seed_resume.json`** - Progress tracking for resume functionality\n",
        "\n",
        "### ‚è±Ô∏è How Long Will This Take?\n",
        "- **Small collection (<100 GIFs)**: 2-5 minutes\n",
        "- **Medium collection (100-1000 GIFs)**: 5-15 minutes  \n",
        "- **Large collection (1000+ GIFs)**: 15-60 minutes\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Step-by-Step Process\n",
        "\n",
        "### **Step 1:** [Setup & Check Environment](#setup)\n",
        "*Get everything ready and make sure paths are correct*\n",
        "\n",
        "### **Step 2:** [Scan Your GIF Collection](#collection)\n",
        "*Look through all your GIFs and extract detailed information*\n",
        "\n",
        "### **Step 3:** [Create Metadata Index](#metadata)\n",
        "*Build a comprehensive catalog of your GIFs*\n",
        "\n",
        "### **Step 4:** [Generate Smart Recommendations](#processing)\n",
        "*Create optimization suggestions based on your specific GIFs*\n",
        "\n",
        "### **Step 5:** [Set Up Resume Tracking](#resume)\n",
        "*Prepare the system to track processing progress*\n",
        "\n",
        "### **Step 6:** [Validate Everything](#validation)\n",
        "*Double-check that all files are correct and consistent*\n",
        "\n",
        "### **Step 7:** [Save and Integrate](#integration)\n",
        "*Save all files and get ready for processing*\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Ready to Build Your Optimization Files?\n",
        "\n",
        "**What to expect:**\n",
        "- ‚úÖ **Clear explanations** of what each step does\n",
        "- ‚úÖ **Progress indicators** so you know how it's going\n",
        "- ‚úÖ **Helpful tips** if things don't go as expected\n",
        "- ‚úÖ **Next steps** when you're done\n",
        "\n",
        "**Before starting:**\n",
        "- Make sure you've run the first notebook (`01_explore_dataset.ipynb`) if you want the best results\n",
        "- Ensure your GIF files are in the `data/raw/` directory\n",
        "- This notebook will work even if you haven't run the analysis notebook first\n",
        "\n",
        "**Let's start building your optimization files!** üëá\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup & Configuration <a id=\"setup\"></a>\n",
        "\n",
        "### Environment Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from tqdm import tqdm\n",
        "import hashlib\n",
        "\n",
        "# GifLab imports\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from giflab import meta, config\n",
        "from giflab.meta import extract_gif_metadata, GifMetadata\n",
        "from giflab.config import CompressionConfig, PathConfig\n",
        "\n",
        "# Configuration\n",
        "SEED_CONFIG = {\n",
        "    'version': '1.0',\n",
        "    'max_files_scan': None,  # None = scan all files\n",
        "    'batch_size': 1000,\n",
        "    'validation_enabled': True,\n",
        "    'backup_existing': True\n",
        "}\n",
        "\n",
        "print(\"üå± GifLab Seed JSON Builder\")\n",
        "print(f\"üîß Configuration: {SEED_CONFIG}\")\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "\n",
        "# Set up paths\n",
        "raw_dir = Path(\"../data/raw\")\n",
        "seed_dir = Path(\"../seed\")\n",
        "csv_dir = Path(\"../data/csv\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "seed_dir.mkdir(exist_ok=True)\n",
        "csv_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(f\"üìÅ Raw directory: {raw_dir}\")\n",
        "print(f\"üìÅ Seed directory: {seed_dir}\")\n",
        "print(f\"üìÅ CSV directory: {csv_dir}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Collection & Hashing <a id=\"collection\"></a>\n",
        "\n",
        "### Comprehensive GIF Directory Scanning and Metadata Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scan_gif_directory_comprehensive(raw_dir: Path, max_files: Optional[int] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Comprehensive scan of GIF directory with metadata extraction.\n",
        "    \n",
        "    Args:\n",
        "        raw_dir: Path to directory containing raw GIF files\n",
        "        max_files: Maximum number of files to process (None = all files)\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing comprehensive file information\n",
        "    \"\"\"\n",
        "    print(f\"üîç Starting comprehensive scan of: {raw_dir}\")\n",
        "    \n",
        "    if not raw_dir.exists():\n",
        "        print(f\"‚ö†Ô∏è  Directory not found: {raw_dir}\")\n",
        "        return {\n",
        "            'scan_completed': False,\n",
        "            'error': f\"Directory not found: {raw_dir}\",\n",
        "            'total_files_found': 0,\n",
        "            'valid_gifs': [],\n",
        "            'corrupted_gifs': [],\n",
        "            'duplicate_groups': {}\n",
        "        }\n",
        "    \n",
        "    # Find all GIF files recursively\n",
        "    gif_files = list(raw_dir.rglob(\"*.gif\"))\n",
        "    total_files = len(gif_files)\n",
        "    \n",
        "    print(f\"üìÅ Found {total_files} GIF files\")\n",
        "    \n",
        "    # Sample files if requested\n",
        "    if max_files and max_files < total_files:\n",
        "        import random\n",
        "        random.seed(42)  # For reproducible sampling\n",
        "        gif_files = random.sample(gif_files, max_files)\n",
        "        print(f\"üìä Sampling {max_files} files for analysis\")\n",
        "    \n",
        "    # Process files\n",
        "    valid_gifs = []\n",
        "    corrupted_gifs = []\n",
        "    sha_to_paths = {}  # For duplicate detection\n",
        "    \n",
        "    for gif_path in tqdm(gif_files, desc=\"Processing GIFs\"):\n",
        "        try:\n",
        "            # Get file stats\n",
        "            stat = gif_path.stat()\n",
        "            file_size_bytes = stat.st_size\n",
        "            file_modified = datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
        "            \n",
        "            # Extract metadata\n",
        "            try:\n",
        "                metadata = extract_gif_metadata(gif_path)\n",
        "                \n",
        "                # Calculate derived metrics\n",
        "                aspect_ratio = metadata.orig_width / metadata.orig_height if metadata.orig_height > 0 else 0\n",
        "                duration_seconds = metadata.orig_frames / metadata.orig_fps if metadata.orig_fps > 0 else 0\n",
        "                pixels_total = metadata.orig_width * metadata.orig_height\n",
        "                \n",
        "                # Calculate complexity score\n",
        "                complexity_score = calculate_complexity_score(metadata, pixels_total)\n",
        "                complexity_category = classify_complexity(complexity_score)\n",
        "                \n",
        "                gif_info = {\n",
        "                    'gif_sha': metadata.gif_sha,\n",
        "                    'orig_filename': metadata.orig_filename,\n",
        "                    'file_path': str(gif_path.relative_to(raw_dir)),\n",
        "                    'absolute_path': str(gif_path),\n",
        "                    'file_size_bytes': file_size_bytes,\n",
        "                    'orig_kilobytes': metadata.orig_kilobytes,\n",
        "                    'orig_width': metadata.orig_width,\n",
        "                    'orig_height': metadata.orig_height,\n",
        "                    'orig_frames': metadata.orig_frames,\n",
        "                    'orig_fps': metadata.orig_fps,\n",
        "                    'orig_n_colors': metadata.orig_n_colors,\n",
        "                    'entropy': metadata.entropy,\n",
        "                    'aspect_ratio': aspect_ratio,\n",
        "                    'duration_seconds': duration_seconds,\n",
        "                    'pixels_total': pixels_total,\n",
        "                    'complexity_score': complexity_score,\n",
        "                    'complexity_category': complexity_category,\n",
        "                    'file_modified': file_modified,\n",
        "                    'processing_priority': get_processing_priority(complexity_category)\n",
        "                }\n",
        "                \n",
        "                valid_gifs.append(gif_info)\n",
        "                \n",
        "                # Track for duplicate detection\n",
        "                sha = metadata.gif_sha\n",
        "                if sha not in sha_to_paths:\n",
        "                    sha_to_paths[sha] = []\n",
        "                sha_to_paths[sha].append(gif_path)\n",
        "                \n",
        "            except Exception as meta_error:\n",
        "                corrupted_gifs.append({\n",
        "                    'file_path': str(gif_path.relative_to(raw_dir)),\n",
        "                    'absolute_path': str(gif_path),\n",
        "                    'file_size_bytes': file_size_bytes,\n",
        "                    'error': str(meta_error),\n",
        "                    'file_modified': file_modified\n",
        "                })\\n                \\n        except Exception as file_error:\\n            print(f\"‚ö†Ô∏è  Error accessing {gif_path}: {file_error}\")\\n    \\n    # Identify duplicate groups\\n    duplicate_groups = {}\\n    for sha, paths in sha_to_paths.items():\\n        if len(paths) > 1:\\n            duplicate_groups[f\"group_{len(duplicate_groups) + 1}\"] = {\\n                'canonical_sha': sha,\\n                'duplicate_paths': [str(p.relative_to(raw_dir)) for p in paths[1:]],\\n                'canonical_path': str(paths[0].relative_to(raw_dir))\\n            }\\n    \\n    return {\\n        'scan_completed': True,\\n        'scan_timestamp': datetime.now().isoformat(),\\n        'total_files_found': total_files,\\n        'files_processed': len(gif_files),\\n        'valid_gifs': valid_gifs,\\n        'corrupted_gifs': corrupted_gifs,\\n        'duplicate_groups': duplicate_groups,\\n        'success_rate': len(valid_gifs) / len(gif_files) if gif_files else 0\\n    }\\n\\ndef calculate_complexity_score(metadata: GifMetadata, pixels_total: int) -> float:\\n    \"\"\"Calculate complexity score for a GIF based on its characteristics.\"\"\"\\n    score = 0\\n    \\n    # Entropy contribution (0-40 points)\\n    entropy = metadata.entropy or 0\\n    if entropy > 6:\\n        score += 40\\n    elif entropy > 4:\\n        score += 25\\n    else:\\n        score += 10\\n    \\n    # Frame count contribution (0-30 points)\\n    frames = metadata.orig_frames\\n    if frames > 50:\\n        score += 30\\n    elif frames > 20:\\n        score += 20\\n    else:\\n        score += 5\\n    \\n    # Dimension contribution (0-30 points)\\n    if pixels_total > 500000:  # ~720p\\n        score += 30\\n    elif pixels_total > 200000:  # ~480p\\n        score += 20\\n    else:\\n        score += 10\\n    \\n    return score\\n\\ndef classify_complexity(score: float) -> str:\\n    \"\"\"Classify complexity based on score.\"\"\"\\n    if score >= 80:\\n        return 'high_complexity'\\n    elif score >= 50:\\n        return 'medium_complexity'\\n    else:\\n        return 'low_complexity'\\n\\ndef get_processing_priority(complexity_category: str) -> str:\\n    \"\"\"Get processing priority based on complexity.\"\"\"\\n    priority_map = {\\n        'high_complexity': 'high',\\n        'medium_complexity': 'medium',\\n        'low_complexity': 'low'\\n    }\\n    return priority_map.get(complexity_category, 'medium')\\n\\n# Perform comprehensive scan\\nprint(\"üöÄ Starting comprehensive GIF directory scan...\")\\nscan_results = scan_gif_directory_comprehensive(raw_dir, max_files=SEED_CONFIG['max_files_scan'])\\n\\nprint(f\"\\nüìä Scan Results Summary:\")\\nprint(f\"   ‚Ä¢ Total files found: {scan_results['total_files_found']}\")\\nprint(f\"   ‚Ä¢ Files processed: {scan_results['files_processed']}\")\\nprint(f\"   ‚Ä¢ Valid GIFs: {len(scan_results['valid_gifs'])}\")\\nprint(f\"   ‚Ä¢ Corrupted files: {len(scan_results['corrupted_gifs'])}\")\\nprint(f\"   ‚Ä¢ Duplicate groups: {len(scan_results['duplicate_groups'])}\")\\nprint(f\"   ‚Ä¢ Success rate: {scan_results['success_rate']:.1%}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Metadata Seed Generation <a id=\"metadata\"></a>\n",
        "\n",
        "### Generate `lookup_seed_metadata.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_metadata_seed(scan_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Generate comprehensive metadata seed file.\"\"\"\n",
        "    \n",
        "    print(\"üîß Generating metadata seed file...\")\n",
        "    \n",
        "    if not scan_results['scan_completed'] or not scan_results['valid_gifs']:\n",
        "        print(\"‚ö†Ô∏è  No valid GIFs found for metadata seed generation\")\n",
        "        return {\n",
        "            'version': SEED_CONFIG['version'],\n",
        "            'generated_at': datetime.now().isoformat(),\n",
        "            'total_gifs': 0,\n",
        "            'gif_metadata': {},\n",
        "            'statistics': {},\n",
        "            'scan_info': scan_results\n",
        "        }\n",
        "    \n",
        "    # Build metadata index\n",
        "    gif_metadata = {}\n",
        "    for gif_info in scan_results['valid_gifs']:\n",
        "        sha = gif_info['gif_sha']\n",
        "        gif_metadata[sha] = {\n",
        "            'orig_filename': gif_info['orig_filename'],\n",
        "            'file_path': gif_info['file_path'],\n",
        "            'orig_kilobytes': gif_info['orig_kilobytes'],\n",
        "            'orig_width': gif_info['orig_width'],\n",
        "            'orig_height': gif_info['orig_height'],\n",
        "            'orig_frames': gif_info['orig_frames'],\n",
        "            'orig_fps': gif_info['orig_fps'],\n",
        "            'orig_n_colors': gif_info['orig_n_colors'],\n",
        "            'entropy': gif_info['entropy'],\n",
        "            'aspect_ratio': gif_info['aspect_ratio'],\n",
        "            'duration_seconds': gif_info['duration_seconds'],\n",
        "            'complexity_score': gif_info['complexity_score'],\n",
        "            'complexity_category': gif_info['complexity_category'],\n",
        "            'processing_priority': gif_info['processing_priority'],\n",
        "            'file_modified': gif_info['file_modified'],\n",
        "            'pixels_total': gif_info['pixels_total']\n",
        "        }\n",
        "    \n",
        "    # Calculate statistics\n",
        "    df = pd.DataFrame(scan_results['valid_gifs'])\n",
        "    \n",
        "    statistics = {\n",
        "        'total_gifs': len(scan_results['valid_gifs']),\n",
        "        'avg_file_size_kb': float(df['orig_kilobytes'].mean()) if not df.empty else 0,\n",
        "        'avg_frames': float(df['orig_frames'].mean()) if not df.empty else 0,\n",
        "        'avg_fps': float(df['orig_fps'].mean()) if not df.empty else 0,\n",
        "        'avg_duration_seconds': float(df['duration_seconds'].mean()) if not df.empty else 0,\n",
        "        'complexity_distribution': df['complexity_category'].value_counts().to_dict() if not df.empty else {},\n",
        "        'most_common_dimensions': [],\n",
        "        'most_common_aspect_ratios': [],\n",
        "        'file_size_percentiles': {},\n",
        "        'entropy_stats': {}\n",
        "    }\n",
        "    \n",
        "    if not df.empty:\n",
        "        # Most common dimensions\n",
        "        dimension_counts = df.apply(lambda x: f\"{x['orig_width']}x{x['orig_height']}\", axis=1).value_counts()\n",
        "        statistics['most_common_dimensions'] = dimension_counts.head(5).index.tolist()\n",
        "        \n",
        "        # Most common aspect ratios (rounded)\n",
        "        aspect_ratio_counts = df['aspect_ratio'].round(2).value_counts()\n",
        "        statistics['most_common_aspect_ratios'] = aspect_ratio_counts.head(5).index.tolist()\n",
        "        \n",
        "        # File size percentiles\n",
        "        size_percentiles = df['orig_kilobytes'].quantile([0.25, 0.5, 0.75, 0.9, 0.95])\n",
        "        statistics['file_size_percentiles'] = {\n",
        "            f'p{int(k*100)}': float(v) for k, v in size_percentiles.items()\n",
        "        }\n",
        "        \n",
        "        # Entropy statistics\n",
        "        statistics['entropy_stats'] = {\n",
        "            'mean': float(df['entropy'].mean()),\n",
        "            'std': float(df['entropy'].std()),\n",
        "            'min': float(df['entropy'].min()),\n",
        "            'max': float(df['entropy'].max())\n",
        "        }\n",
        "    \n",
        "    metadata_seed = {\n",
        "        'version': SEED_CONFIG['version'],\n",
        "        'generated_at': datetime.now().isoformat(),\n",
        "        'total_gifs': len(scan_results['valid_gifs']),\n",
        "        'gif_metadata': gif_metadata,\n",
        "        'statistics': statistics,\n",
        "        'scan_info': {\n",
        "            'total_files_found': scan_results['total_files_found'],\n",
        "            'files_processed': scan_results['files_processed'],\n",
        "            'corrupted_files': len(scan_results['corrupted_gifs']),\n",
        "            'duplicate_groups': len(scan_results['duplicate_groups']),\n",
        "            'success_rate': scan_results['success_rate']\n",
        "                 }\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Generated metadata seed for {len(gif_metadata)} GIFs\")\n",
        "    return metadata_seed\n",
        "\n",
        "# Generate metadata seed\n",
        "metadata_seed = generate_metadata_seed(scan_results)\n",
        "\n",
        "print(f\"\\nüìä Metadata Seed Summary:\")\n",
        "print(f\"   ‚Ä¢ Total GIFs indexed: {metadata_seed['total_gifs']}\")\n",
        "if metadata_seed['total_gifs'] > 0:\n",
        "    stats = metadata_seed['statistics']\n",
        "    print(f\"   ‚Ä¢ Average file size: {stats['avg_file_size_kb']:.1f} KB\")\n",
        "    print(f\"   ‚Ä¢ Average frames: {stats['avg_frames']:.1f}\")\n",
        "    print(f\"   ‚Ä¢ Average FPS: {stats['avg_fps']:.1f}\")\n",
        "    print(f\"   ‚Ä¢ Complexity distribution: {stats['complexity_distribution']}\")\n",
        "    print(f\"   ‚Ä¢ Common dimensions: {stats['most_common_dimensions'][:3]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Processing Optimization Seed <a id=\"processing\"></a>\n",
        "\n",
        "### Generate `lookup_seed_processing.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_processing_seed(scan_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Generate processing optimization seed file.\"\"\"\n",
        "    \n",
        "    print(\"üîß Generating processing optimization seed file...\")\n",
        "    \n",
        "    if not scan_results['scan_completed'] or not scan_results['valid_gifs']:\n",
        "        print(\"‚ö†Ô∏è  No valid GIFs found for processing seed generation\")\n",
        "        return {\n",
        "            'version': SEED_CONFIG['version'],\n",
        "            'generated_at': datetime.now().isoformat(),\n",
        "            'processing_batches': {},\n",
        "            'parameter_recommendations': {},\n",
        "            'duplicate_groups': scan_results.get('duplicate_groups', {})\n",
        "        }\n",
        "    \n",
        "    # Group GIFs by complexity\n",
        "    complexity_groups = {\n",
        "        'high_complexity': [],\n",
        "        'medium_complexity': [],\n",
        "        'low_complexity': []\n",
        "    }\n",
        "    \n",
        "    for gif_info in scan_results['valid_gifs']:\n",
        "        category = gif_info['complexity_category']\n",
        "        complexity_groups[category].append(gif_info['gif_sha'])\n",
        "    \n",
        "    # Define processing characteristics and recommendations\n",
        "    processing_batches = {}\n",
        "    for category, gifs in complexity_groups.items():\n",
        "        if gifs:  # Only include categories with GIFs\n",
        "            if category == 'high_complexity':\n",
        "                characteristics = \"High entropy, many frames, large dimensions\"\n",
        "                batch_size = 50\n",
        "                estimated_time = 45.2\n",
        "            elif category == 'medium_complexity':\n",
        "                characteristics = \"Moderate entropy, average frames\"\n",
        "                batch_size = 100\n",
        "                estimated_time = 22.8\n",
        "            else:  # low_complexity\n",
        "                characteristics = \"Low entropy, few frames, small dimensions\"\n",
        "                batch_size = 200\n",
        "                estimated_time = 8.5\n",
        "            \n",
        "            processing_batches[category] = {\n",
        "                'gifs': gifs,\n",
        "                'count': len(gifs),\n",
        "                'characteristics': characteristics,\n",
        "                'recommended_batch_size': batch_size,\n",
        "                'estimated_time_per_gif_seconds': estimated_time\n",
        "            }\n",
        "    \n",
        "    # Generate parameter recommendations for each GIF\n",
        "    parameter_recommendations = {}\n",
        "    for gif_info in scan_results['valid_gifs']:\n",
        "        sha = gif_info['gif_sha']\n",
        "        category = gif_info['complexity_category']\n",
        "        \n",
        "        # Define recommendations based on complexity\n",
        "        if category == 'high_complexity':\n",
        "            rec = {\n",
        "                'optimal_engines': ['gifsicle', 'animately'],\n",
        "                'recommended_lossy': [0, 40],\n",
        "                'recommended_frame_ratios': [1.0, 0.8, 0.6, 0.4],\n",
        "                'recommended_colors': [256, 128, 64],\n",
        "                'skip_combinations': [\n",
        "                    {'engine': 'animately', 'lossy': 120, 'reason': 'poor_quality_on_complex_gifs'}\n",
        "                ],\n",
        "                'quality_target_ssim': 0.9\n",
        "            }\n",
        "        elif category == 'medium_complexity':\n",
        "            rec = {\n",
        "                'optimal_engines': ['gifsicle', 'animately'],\n",
        "                'recommended_lossy': [0, 40, 80],\n",
        "                'recommended_frame_ratios': [1.0, 0.8, 0.6],\n",
        "                'recommended_colors': [256, 128, 64],\n",
        "                'skip_combinations': [],\n",
        "                'quality_target_ssim': 0.8\n",
        "            }\n",
        "        else:  # low_complexity\n",
        "            rec = {\n",
        "                'optimal_engines': ['gifsicle', 'animately'],\n",
        "                'recommended_lossy': [0, 40, 80, 120],\n",
        "                'recommended_frame_ratios': [1.0, 0.8, 0.6, 0.4],\n",
        "                'recommended_colors': [256, 128, 64],\n",
        "                'skip_combinations': [],\n",
        "                'quality_target_ssim': 0.7\n",
        "            }\n",
        "        \n",
        "        # Add GIF-specific context\n",
        "        rec['gif_context'] = {\n",
        "            'complexity_score': gif_info['complexity_score'],\n",
        "            'orig_frames': gif_info['orig_frames'],\n",
        "            'orig_fps': gif_info['orig_fps'],\n",
        "            'entropy': gif_info['entropy'],\n",
        "            'pixels_total': gif_info['pixels_total']\n",
        "        }\n",
        "        \n",
        "        parameter_recommendations[sha] = rec\n",
        "    \n",
        "    processing_seed = {\n",
        "        'version': SEED_CONFIG['version'],\n",
        "        'generated_at': datetime.now().isoformat(),\n",
        "        'processing_batches': processing_batches,\n",
        "        'parameter_recommendations': parameter_recommendations,\n",
        "        'duplicate_groups': scan_results.get('duplicate_groups', {}),\n",
        "        'batch_summary': {\n",
        "            'total_gifs': len(scan_results['valid_gifs']),\n",
        "            'high_complexity_count': len(complexity_groups['high_complexity']),\n",
        "            'medium_complexity_count': len(complexity_groups['medium_complexity']),\n",
        "            'low_complexity_count': len(complexity_groups['low_complexity'])\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Generated processing seed for {len(parameter_recommendations)} GIFs\")\n",
        "    return processing_seed\n",
        "\n",
        "# Generate processing seed\n",
        "processing_seed = generate_processing_seed(scan_results)\n",
        "\n",
        "print(f\"\\nüìä Processing Seed Summary:\")\n",
        "print(f\"   ‚Ä¢ Total GIFs with recommendations: {len(processing_seed['parameter_recommendations'])}\")\n",
        "print(f\"   ‚Ä¢ Processing batches: {len(processing_seed['processing_batches'])}\")\n",
        "print(f\"   ‚Ä¢ Duplicate groups: {len(processing_seed['duplicate_groups'])}\")\n",
        "\n",
        "if processing_seed['processing_batches']:\n",
        "    print(f\"   ‚Ä¢ Batch distribution:\")\n",
        "    for category, batch_info in processing_seed['processing_batches'].items():\n",
        "        print(f\"     - {category}: {batch_info['count']} GIFs (batch size: {batch_info['recommended_batch_size']})\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Resume State Management <a id=\"resume\"></a>\n",
        "\n",
        "### Generate `lookup_seed_resume.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_resume_state(csv_files: List[Path], scan_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Generate resume state from existing CSV files and scan results.\"\"\"\n",
        "    \n",
        "    print(\"üîß Generating resume state seed file...\")\n",
        "    \n",
        "    # Initialize resume state structure\n",
        "    resume_state = {\n",
        "        'version': SEED_CONFIG['version'],\n",
        "        'last_updated': datetime.now().isoformat(),\n",
        "        'processing_sessions': {},\n",
        "        'completed_jobs': {},\n",
        "        'failed_jobs': {},\n",
        "        'progress_summary': {\n",
        "            'total_gifs': len(scan_results.get('valid_gifs', [])),\n",
        "            'completed_gifs': 0,\n",
        "            'failed_gifs': len(scan_results.get('corrupted_gifs', [])),\n",
        "            'pending_gifs': 0,\n",
        "            'completion_percentage': 0.0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Add corrupted files to failed jobs\n",
        "    for corrupted_gif in scan_results.get('corrupted_gifs', []):\n",
        "        file_path = corrupted_gif['file_path']\n",
        "        resume_state['failed_jobs'][file_path] = {\n",
        "            'error': corrupted_gif['error'],\n",
        "            'attempts': 1,\n",
        "            'last_attempt': datetime.now().isoformat(),\n",
        "            'moved_to_bad_gifs': False,\n",
        "            'file_size_bytes': corrupted_gif.get('file_size_bytes', 0)\n",
        "        }\n",
        "    \n",
        "    # Process existing CSV files to determine completion state\n",
        "    all_completed_jobs = {}\n",
        "    \n",
        "    for csv_file in csv_files:\n",
        "        if csv_file.exists():\n",
        "            try:\n",
        "                print(f\"üìÑ Processing CSV file: {csv_file}\")\n",
        "                 df = pd.read_csv(csv_file)\n",
        "                \n",
        "                for _, row in df.iterrows():\n",
        "                    gif_sha = row['gif_sha']\n",
        "                    engine = row['engine']\n",
        "                    \n",
        "                    if gif_sha not in all_completed_jobs:\n",
        "                        all_completed_jobs[gif_sha] = {}\n",
        "                    \n",
        "                    if engine not in all_completed_jobs[gif_sha]:\n",
        "                        all_completed_jobs[gif_sha][engine] = {\n",
        "                            'completed_variants': [],\n",
        "                            'pending_variants': []\n",
        "                        }\n",
        "                    \n",
        "                    variant = {\n",
        "                        'lossy': int(row['lossy']),\n",
        "                        'frame_ratio': float(row['frame_keep_ratio']),\n",
        "                        'colors': int(row['color_keep_count']),\n",
        "                        'timestamp': row.get('timestamp', datetime.now().isoformat()),\n",
        "                        'kilobytes': float(row['kilobytes']),\n",
        "                        'ssim': float(row['ssim']) if pd.notna(row.get('ssim')) else None\n",
        "                    }\n",
        "                    \n",
        "                     all_completed_jobs[gif_sha][engine]['completed_variants'].append(variant)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error processing CSV {csv_file}: {e}\")\\n    \\n    # Update resume state with completed jobs\\n    resume_state['completed_jobs'] = all_completed_jobs\\n    \\n    # Calculate progress summary\\n    total_gifs = len(scan_results.get('valid_gifs', []))\\n    completed_gifs = len(all_completed_jobs)\\n    failed_gifs = len(scan_results.get('corrupted_gifs', []))\\n    pending_gifs = max(0, total_gifs - completed_gifs)\\n    \\n    resume_state['progress_summary'] = {\\n        'total_gifs': total_gifs,\\n        'completed_gifs': completed_gifs,\\n        'failed_gifs': failed_gifs,\\n        'pending_gifs': pending_gifs,\\n        'completion_percentage': (completed_gifs / total_gifs * 100) if total_gifs > 0 else 0.0\\n    }\\n    \\n    # Generate pending variants for incomplete GIFs\\n    compression_config = CompressionConfig()\\n    \\n    for gif_info in scan_results.get('valid_gifs', []):\\n        gif_sha = gif_info['gif_sha']\\n        \\n        if gif_sha not in all_completed_jobs:\\n            all_completed_jobs[gif_sha] = {}\\n        \\n        # Check what variants are missing for each engine\\n        for engine in compression_config.ENGINES:\\n            if engine not in all_completed_jobs[gif_sha]:\\n                all_completed_jobs[gif_sha][engine] = {\\n                    'completed_variants': [],\\n                    'pending_variants': []\\n                }\\n            \\n            completed_variants = all_completed_jobs[gif_sha][engine]['completed_variants']\\n            completed_combinations = set()\\n            \\n            for variant in completed_variants:\\n                combo = (variant['lossy'], variant['frame_ratio'], variant['colors'])\\n                completed_combinations.add(combo)\\n            \\n            # Generate all possible combinations\\n            all_combinations = set()\\n            for lossy in compression_config.LOSSY_LEVELS:\\n                for ratio in compression_config.FRAME_KEEP_RATIOS:\\n                    for colors in compression_config.COLOR_KEEP_COUNTS:\\n                        all_combinations.add((lossy, ratio, colors))\\n            \\n            # Find pending combinations\\n            pending_combinations = all_combinations - completed_combinations\\n            \\n            pending_variants = []\\n            for lossy, ratio, colors in pending_combinations:\\n                pending_variants.append({\\n                    'lossy': lossy,\\n                    'frame_ratio': ratio,\\n                    'colors': colors\\n                })\\n            \\n            all_completed_jobs[gif_sha][engine]['pending_variants'] = pending_variants\\n    \\n    resume_state['completed_jobs'] = all_completed_jobs\\n    \\n    print(f\"‚úÖ Generated resume state for {total_gifs} GIFs\")\\n    return resume_state\\n\\n# Find existing CSV files\\ncsv_files = list(csv_dir.glob(\"*.csv\")) if csv_dir.exists() else []\\nprint(f\"üìÑ Found {len(csv_files)} existing CSV files\")\\n\\n# Generate resume state\\nresume_seed = generate_resume_state(csv_files, scan_results)\\n\\nprint(f\"\\nüìä Resume State Summary:\")\\nprint(f\"   ‚Ä¢ Total GIFs: {resume_seed['progress_summary']['total_gifs']}\")\\nprint(f\"   ‚Ä¢ Completed GIFs: {resume_seed['progress_summary']['completed_gifs']}\")\\nprint(f\"   ‚Ä¢ Failed GIFs: {resume_seed['progress_summary']['failed_gifs']}\")\\nprint(f\"   ‚Ä¢ Pending GIFs: {resume_seed['progress_summary']['pending_gifs']}\")\\nprint(f\"   ‚Ä¢ Completion: {resume_seed['progress_summary']['completion_percentage']:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Validation & Quality Checks <a id=\"validation\"></a>\n",
        "\n",
        "### Validate Seed Data Integrity and Consistency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_seed_data(seed_files: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"Validate integrity and consistency of all seed files.\"\"\"\n",
        "    \n",
        "    print(\"üîç Validating seed data integrity...\")\n",
        "    \n",
        "    validation_results = {\n",
        "        'overall_valid': True,\n",
        "        'validation_timestamp': datetime.now().isoformat(),\n",
        "        'file_validations': {},\n",
        "        'cross_validation': {},\n",
        "        'recommendations': []\n",
        "    }\n",
        "    \n",
        "    # Validate each seed file individually\n",
        "    for seed_name, seed_data in seed_files.items():\n",
        "        print(f\"   Validating {seed_name}...\")\n",
        "        \n",
        "        file_validation = {\n",
        "            'valid': True,\n",
        "            'errors': [],\n",
        "            'warnings': [],\n",
        "            'stats': {}\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            # Check required fields\n",
        "            required_fields = ['version', 'generated_at']\n",
        "            for field in required_fields:\n",
        "                if field not in seed_data:\n",
        "                    file_validation['errors'].append(f\"Missing required field: {field}\")\n",
        "                    file_validation['valid'] = False\n",
        "            \n",
        "            # Validate version consistency\n",
        "            if 'version' in seed_data and seed_data['version'] != SEED_CONFIG['version']:\n",
        "                file_validation['warnings'].append(f\"Version mismatch: {seed_data['version']} != {SEED_CONFIG['version']}\")\n",
        "            \n",
        "            # Seed-specific validations\n",
        "            if seed_name == 'metadata':\n",
        "                if 'gif_metadata' in seed_data:\n",
        "                    gif_count = len(seed_data['gif_metadata'])\n",
        "                    file_validation['stats']['gif_count'] = gif_count\n",
        "                    \n",
        "                    if gif_count == 0:\n",
        "                        file_validation['warnings'].append(\"No GIF metadata found\")\n",
        "                    \n",
        "                    # Check for required metadata fields\n",
        "                    if gif_count > 0:\n",
        "                        sample_gif = next(iter(seed_data['gif_metadata'].values()))\n",
        "                        required_gif_fields = ['orig_filename', 'file_path', 'orig_kilobytes']\n",
        "                        for field in required_gif_fields:\n",
        "                            if field not in sample_gif:\n",
        "                                file_validation['errors'].append(f\"Missing GIF field: {field}\")\n",
        "                                file_validation['valid'] = False\n",
        "            \n",
        "            elif seed_name == 'processing':\n",
        "                if 'parameter_recommendations' in seed_data:\n",
        "                    rec_count = len(seed_data['parameter_recommendations'])\n",
        "                    file_validation['stats']['recommendation_count'] = rec_count\n",
        "                    \n",
        "                    if rec_count == 0:\n",
        "                        file_validation['warnings'].append(\"No parameter recommendations found\")\n",
        "            \n",
        "            elif seed_name == 'resume':\n",
        "                if 'progress_summary' in seed_data:\n",
        "                    progress = seed_data['progress_summary']\n",
        "                    file_validation['stats']['completion_percentage'] = progress.get('completion_percentage', 0)\n",
        "                    \n",
        "                    # Check progress consistency\n",
        "                    total = progress.get('total_gifs', 0)\n",
        "                    completed = progress.get('completed_gifs', 0)\n",
        "                    failed = progress.get('failed_gifs', 0)\n",
        "                    pending = progress.get('pending_gifs', 0)\n",
        "                    \n",
        "                    if total != completed + failed + pending:\n",
        "                        file_validation['errors'].append(\"Progress counts don't add up\")\n",
        "                        file_validation['valid'] = False\n",
        "        \n",
        "        except Exception as e:\n",
        "            file_validation['errors'].append(f\"Validation error: {str(e)}\")\n",
        "            file_validation['valid'] = False\n",
        "        \n",
        "        validation_results['file_validations'][seed_name] = file_validation\n",
        "        \n",
        "        if not file_validation['valid']:\n",
        "            validation_results['overall_valid'] = False\n",
        "    \n",
        "    # Cross-validation between seed files\n",
        "    print(\"   Performing cross-validation...\")\n",
        "    \n",
        "    try:\n",
        "        metadata_gifs = set()\n",
        "        processing_gifs = set()\n",
        "        resume_gifs = set()\n",
        "        \n",
        "        if 'metadata' in seed_files and 'gif_metadata' in seed_files['metadata']:\n",
        "            metadata_gifs = set(seed_files['metadata']['gif_metadata'].keys())\n",
        "        \n",
        "        if 'processing' in seed_files and 'parameter_recommendations' in seed_files['processing']:\n",
        "            processing_gifs = set(seed_files['processing']['parameter_recommendations'].keys())\n",
        "        \n",
        "        if 'resume' in seed_files and 'completed_jobs' in seed_files['resume']:\n",
        "            resume_gifs = set(seed_files['resume']['completed_jobs'].keys())\n",
        "        \n",
        "        # Check consistency between files\n",
        "        cross_validation = {\n",
        "            'metadata_processing_match': metadata_gifs == processing_gifs,\n",
        "            'metadata_count': len(metadata_gifs),\n",
        "            'processing_count': len(processing_gifs),\n",
        "            'resume_count': len(resume_gifs),\n",
        "            'missing_in_processing': list(metadata_gifs - processing_gifs),\n",
        "            'missing_in_metadata': list(processing_gifs - metadata_gifs)\n",
        "        }\n",
        "        \n",
        "        validation_results['cross_validation'] = cross_validation\n",
        "        \n",
        "        if not cross_validation['metadata_processing_match']:\n",
        "            validation_results['recommendations'].append(\"Metadata and processing seed files have different GIF sets\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        validation_results['cross_validation']['error'] = str(e)\n",
        "    \n",
        "    # Generate recommendations\n",
        "    if validation_results['overall_valid']:\n",
        "        validation_results['recommendations'].append(\"All seed files are valid and consistent\")\n",
        "    else:\n",
        "        validation_results['recommendations'].append(\"Fix validation errors before using seed files\")\n",
        "    \n",
        "    return validation_results\n",
        "\n",
        "# Prepare seed files for validation\n",
        "seed_files_for_validation = {}\n",
        "\n",
        "if 'metadata_seed' in locals() and metadata_seed:\n",
        "    seed_files_for_validation['metadata'] = metadata_seed\n",
        "\n",
        "if 'processing_seed' in locals() and processing_seed:\n",
        "    seed_files_for_validation['processing'] = processing_seed\n",
        "\n",
        "if 'resume_seed' in locals() and resume_seed:\n",
        "    seed_files_for_validation['resume'] = resume_seed\n",
        "\n",
        "# Perform validation\n",
        "if seed_files_for_validation:\n",
        "    validation_results = validate_seed_data(seed_files_for_validation)\n",
        "    \n",
        "    print(f\"\\nüìã Validation Results:\")\n",
        "    print(f\"   ‚Ä¢ Overall valid: {validation_results['overall_valid']}\")\n",
        "    \n",
        "    for seed_name, file_val in validation_results['file_validations'].items():\n",
        "        status = \"‚úÖ\" if file_val['valid'] else \"‚ùå\"\n",
        "        print(f\"   ‚Ä¢ {seed_name}: {status}\")\n",
        "        \n",
        "        if file_val['errors']:\n",
        "            for error in file_val['errors']:\n",
        "                print(f\"     - Error: {error}\")\n",
        "        \n",
        "        if file_val['warnings']:\n",
        "            for warning in file_val['warnings']:\n",
        "                print(f\"     - Warning: {warning}\")\n",
        "    \n",
        "    # Cross-validation results\n",
        "    if validation_results['cross_validation']:\n",
        "        cv = validation_results['cross_validation']\n",
        "        print(f\"\\nüîó Cross-validation:\")\n",
        "        print(f\"   ‚Ä¢ Metadata-Processing match: {cv.get('metadata_processing_match', 'Unknown')}\")\n",
        "        print(f\"   ‚Ä¢ Metadata GIFs: {cv.get('metadata_count', 0)}\")\n",
        "        print(f\"   ‚Ä¢ Processing GIFs: {cv.get('processing_count', 0)}\")\n",
        "        print(f\"   ‚Ä¢ Resume GIFs: {cv.get('resume_count', 0)}\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"\\nüí° Recommendations:\")\n",
        "    for rec in validation_results['recommendations']:\n",
        "        print(f\"   ‚Ä¢ {rec}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No seed files available for validation\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Integration & Export <a id=\"integration\"></a>\n",
        "\n",
        "### Save Seed Files and Generate Integration Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_seed_files(seed_files: Dict[str, Dict[str, Any]], seed_dir: Path, backup_existing: bool = True) -> Dict[str, Path]:\n",
        "    \"\"\"Save all seed files to the seed directory.\"\"\"\n",
        "    \n",
        "    print(\"üíæ Saving seed files...\")\n",
        "    \n",
        "    saved_files = {}\n",
        "    \n",
        "    # Define seed file mappings\n",
        "    seed_file_names = {\n",
        "        'metadata': 'lookup_seed_metadata.json',\n",
        "        'processing': 'lookup_seed_processing.json',\n",
        "        'resume': 'lookup_seed_resume.json'\n",
        "    }\n",
        "    \n",
        "    for seed_type, seed_data in seed_files.items():\n",
        "        if seed_type not in seed_file_names:\n",
        "            print(f\"‚ö†Ô∏è  Unknown seed type: {seed_type}\")\n",
        "            continue\n",
        "        \n",
        "        file_name = seed_file_names[seed_type]\n",
        "        file_path = seed_dir / file_name\n",
        "        \n",
        "        try:\n",
        "            # Backup existing file if requested\n",
        "            if backup_existing and file_path.exists():\n",
        "                backup_path = seed_dir / f\"{file_path.stem}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "                file_path.rename(backup_path)\n",
        "                print(f\"   üì¶ Backed up existing {file_name} to {backup_path.name}\")\n",
        "            \n",
        "            # Save new seed file\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(seed_data, f, indent=2, ensure_ascii=False, default=str)\n",
        "            \n",
        "            saved_files[seed_type] = file_path\n",
        "            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "            print(f\"   ‚úÖ Saved {file_name} ({file_size_mb:.2f} MB)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error saving {file_name}: {e}\")\n",
        "    \n",
        "    return saved_files\n",
        "\n",
        "def generate_integration_summary(saved_files: Dict[str, Path], validation_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Generate summary for integration with the main pipeline.\"\"\"\n",
        "    \n",
        "    summary = {\n",
        "        'generation_timestamp': datetime.now().isoformat(),\n",
        "        'seed_files': {},\n",
        "        'integration_ready': validation_results.get('overall_valid', False),\n",
        "        'usage_instructions': {},\n",
        "        'next_steps': []\n",
        "    }\n",
        "    \n",
        "    # Document saved files\n",
        "    for seed_type, file_path in saved_files.items():\n",
        "        file_stat = file_path.stat()\n",
        "        summary['seed_files'][seed_type] = {\n",
        "            'file_path': str(file_path),\n",
        "            'file_size_bytes': file_stat.st_size,\n",
        "            'file_size_mb': file_stat.st_size / (1024 * 1024),\n",
        "            'last_modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat()\n",
        "        }\n",
        "    \n",
        "    # Usage instructions\n",
        "    summary['usage_instructions'] = {\n",
        "        'metadata_seed': \"Use for GIF metadata lookup, deduplication, and complexity classification\",\n",
        "        'processing_seed': \"Use for parameter recommendations and batch optimization\",\n",
        "        'resume_seed': \"Use for tracking processing progress and resume functionality\",\n",
        "        'pipeline_integration': \"Import seed files in pipeline.py for efficient processing\"\n",
        "    }\n",
        "    \n",
        "    # Next steps based on validation results\n",
        "    if validation_results.get('overall_valid', False):\n",
        "        summary['next_steps'] = [\n",
        "            \"Integrate seed files with compression pipeline\",\n",
        "            \"Test resume functionality with sample GIFs\",\n",
        "            \"Monitor processing performance with complexity-based batching\",\n",
        "            \"Update seed files as new GIFs are processed\"\n",
        "        ]\n",
        "    else:\n",
        "        summary['next_steps'] = [\n",
        "            \"Fix validation errors in seed files\",\n",
        "            \"Re-run seed generation after corrections\",\n",
        "            \"Verify data consistency before pipeline integration\"\n",
        "        ]\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# Prepare seed files for saving\n",
        "seed_files_to_save = {}\n",
        "\n",
        "if 'metadata_seed' in locals() and metadata_seed:\n",
        "    seed_files_to_save['metadata'] = metadata_seed\n",
        "\n",
        "if 'processing_seed' in locals() and processing_seed:\n",
        "    seed_files_to_save['processing'] = processing_seed\n",
        "\n",
        "if 'resume_seed' in locals() and resume_seed:\n",
        "    seed_files_to_save['resume'] = resume_seed\n",
        "\n",
        "# Save seed files\n",
        "if seed_files_to_save:\n",
        "    saved_files = save_seed_files(seed_files_to_save, seed_dir, backup_existing=SEED_CONFIG['backup_existing'])\n",
        "    \n",
        "    # Generate integration summary\n",
        "    integration_summary = generate_integration_summary(saved_files, validation_results if 'validation_results' in locals() else {})\n",
        "    \n",
        "    # Save integration summary\n",
        "    summary_path = seed_dir / 'seed_generation_summary.json'\n",
        "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(integration_summary, f, indent=2, ensure_ascii=False, default=str)\n",
        "    \n",
        "    print(f\"\\nüìä Integration Summary:\")\n",
        "    print(f\"   ‚Ä¢ Seed files generated: {len(saved_files)}\")\n",
        "    print(f\"   ‚Ä¢ Integration ready: {integration_summary['integration_ready']}\")\n",
        "    print(f\"   ‚Ä¢ Summary saved to: {summary_path}\")\n",
        "    \n",
        "    # Display file details\n",
        "    print(f\"\\nüìÅ Generated Files:\")\n",
        "    for seed_type, file_info in integration_summary['seed_files'].items():\n",
        "        print(f\"   ‚Ä¢ {seed_type}: {file_info['file_path']} ({file_info['file_size_mb']:.2f} MB)\")\n",
        "    \n",
        "    # Next steps\n",
        "    print(f\"\\nüöÄ Next Steps:\")\n",
        "    for step in integration_summary['next_steps']:\n",
        "        print(f\"   ‚Ä¢ {step}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No seed files to save\")\n",
        "\n",
        "print(f\"\\n‚úÖ Seed JSON generation complete!\")\n",
        "print(f\"üìÇ All files saved to: {seed_dir}\")\n",
        "print(f\"üîó Ready for integration with GifLab pipeline\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üå± GifLab Seed JSON Builder\n",
        "\n",
        "This notebook builds lookup seed JSON files for efficient dataset processing.\n",
        "\n",
        "**TODO**: This will be implemented in Stage 10 (S10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# GifLab imports  \n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from giflab import meta, io\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Seed JSON Generation\n",
        "\n",
        "Generate lookup_seed_*.json files for dataset indexing and deduplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement seed JSON building\n",
        "print(\"Seed JSON builder notebook - to be implemented in Stage 10\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
